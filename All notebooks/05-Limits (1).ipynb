{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c067993d",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2022/)    \n",
    "## 1MS041, 2022 \n",
    "&copy;2022 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38bfdc",
   "metadata": {},
   "source": [
    "## Limits\n",
    "\n",
    "We demonstrated these ideas using samples simulated from a $Bernoulli$ process with a secret $\\theta^*$.  We had an interactive plot of the likelihood function where we could increase $n$, the number of simulated samples or the amount of data we had to base our estimate on, and see the effect on the shape of the likelihood function.  The animation belows shows the changing likelihood function for the Bernoulli process with unknown $\\theta^*$ as $n$  (the amount of data) increases.\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Likelihood function for Bernoulli process, as $n$ goes from 1 to 1000 in a continuous loop.</th>\n",
    "  </tr>\n",
    "<tr>\n",
    "    <th><img src=\"https://cdn-images-1.medium.com/fit/t/1600/480/1*jxZFpWtCbD4jHPV0DdDuMA.gif\" width=\"300\"></img></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "For large $n$, you can probably make your own guess about the true value of $\\theta^*$ even without knowing $t_n$.  As the animation progresses, we can see the likelihood function 'homing in' on $\\theta = 0.3$. \n",
    "\n",
    "We can see this in another way, by just looking at the sample mean as $n$ increases.  An easy way to do this is with running means:  generate a very large sample and then calculate the mean first over just the first observation in the sample, then the first two, first three, etc etc (running means were discussed in an earlier worksheet if you want to go back and review them in detail in your own time).  Here we just define a function so that we can easily generate sequences of running means for our $Bernoulli$ process with the unknown $\\theta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599592c4",
   "metadata": {},
   "source": [
    "#### Preparation: Let's just evaluate the next cell and focus on concepts.\n",
    "\n",
    "You can see what they are as you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3d0a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def likelihoodBernoulli(theta, n, tStatistic):\n",
    "    '''Bernoulli likelihood function.\n",
    "    theta in [0,1] is the theta to evaluate the likelihood at.\n",
    "    n is the number of observations.\n",
    "    tStatistic is the sum of the n Bernoulli observations.\n",
    "    return a value for the likelihood of theta given the n observations and tStatistic.'''\n",
    "    retValue = 0 # default return value\n",
    "    if (theta >= 0 and theta <= 1): # check on theta\n",
    "        mpfrTheta = RR(theta) # make sure we use a Sage mpfr \n",
    "        retValue = (mpfrTheta^tStatistic)*(1-mpfrTheta)^(n-tStatistic)\n",
    "    return retValue\n",
    "    \n",
    "def bernoulliFInverse(u, theta):\n",
    "    '''A function to evaluate the inverse CDF of a bernoulli.\n",
    "    \n",
    "    Param u is the value to evaluate the inverse CDF at.\n",
    "    Param theta is the distribution parameters.\n",
    "    Returns inverse CDF under theta evaluated at u'''\n",
    "    \n",
    "    return floor(u + theta)\n",
    "    \n",
    "def bernoulliSample(n, theta, simSeed=None):\n",
    "    '''A function to simulate samples from a bernoulli distribution.\n",
    "    \n",
    "    Param n is the number of samples to simulate.\n",
    "    Param theta is the bernoulli distribution parameter.\n",
    "    Param simSeed is a seed for the random number generator, defaulting to 30.\n",
    "    Returns a simulated Bernoulli sample as a list.'''\n",
    "    \n",
    "    np.random.set_seed(simSeed)\n",
    "    us = [np.random() for i in range(n)]\n",
    "    np.random.set_seed(None)\n",
    "    return [bernoulliFInverse(u, theta) for u in us] # use bernoulliFInverse in a list comprehension\n",
    "    \n",
    "def bernoulliSampleSecretTheta(n, theta=0.30, simSeed=30):\n",
    "    '''A function to simulate samples from a bernoulli distribution.\n",
    "    \n",
    "    Param n is the number of samples to simulate.\n",
    "    Param theta is the bernoulli distribution parameter.\n",
    "    Param simSeed is a seed for the random number generator, defaulting to 30.\n",
    "    Returns a simulated Bernoulli sample as a list.'''\n",
    "    \n",
    "    np.random.seed(simSeed)\n",
    "    us = [np.random() for i in range(n)]\n",
    "    np.random.seed(None)\n",
    "    return [bernoulliFInverse(u, theta) for u in us] # use bernoulliFInverse in a list comprehension\n",
    "\n",
    "def bernoulliRunningMeans(n, myTheta, mySeed = None):\n",
    "    '''Function to give a list of n running means from bernoulli with specified theta.\n",
    "    \n",
    "    Param n is the number of running means to generate.\n",
    "    Param myTheta is the theta for the Bernoulli distribution\n",
    "    Param mySeed is a value for the seed of the random number generator, defaulting to None.'''\n",
    "     \n",
    "    sample = bernoulliSample(n, theta=myTheta, simSeed = mySeed)\n",
    "    from pylab import cumsum # we can import in the middle of code\n",
    "    csSample = list(cumsum(sample))\n",
    "    samplesizes = range(1, n+1,1)\n",
    "    return [RR(csSample[i])/samplesizes[i] for i in range(n)]\n",
    "    \n",
    "#return a plot object for BernoulliLikelihood using the secret theta bernoulli generator\n",
    "def plotBernoulliLikelihoodSecretTheta(n):\n",
    "    '''Return a plot object for BernoulliLikelihood using the secret theta bernoulli generator.\n",
    "    \n",
    "    Param n is the number of simulated samples to generate and do likelihood plot for.'''\n",
    "    \n",
    "    thisBSample = bernoulliSampleSecretTheta(n) # make sample\n",
    "    tn = sum(thisBSample) # summary statistic\n",
    "    from pylab import arange\n",
    "    ths = arange(0,1,0.01) # get some values to plot against\n",
    "    liks = [likelihoodBernoulli(t,n,tn) for t in ths] # use the likelihood function to generate likelihoods\n",
    "    redshade = 1*n/1000 # fancy colours\n",
    "    blueshade = 1 - redshade\n",
    "    return line(zip(ths, liks), rgbcolor = (redshade, 0, blueshade))\n",
    "    \n",
    "def cauchyFInverse(u):\n",
    "    '''A function to evaluate the inverse CDF of a standard Cauchy distribution.\n",
    "    \n",
    "    Param u is the value to evaluate the inverse CDF at.'''\n",
    "    \n",
    "    return RR(tan(pi*(u-0.5)))\n",
    "    \n",
    "def cauchySample(n):\n",
    "    '''A function to simulate samples from a standard Cauchy distribution.\n",
    "    \n",
    "    Param n is the number of samples to simulate.'''\n",
    "    \n",
    "    us = [random() for i in range(n)]\n",
    "    return [cauchyFInverse(u) for u in us]\n",
    "\n",
    "def cauchyRunningMeans(n):\n",
    "    '''Function to give a list of n running means from standardCauchy.\n",
    "    \n",
    "    Param n is the number of running means to generate.'''\n",
    "    \n",
    "    sample = cauchySample(n)\n",
    "    from pylab import cumsum\n",
    "    csSample = list(cumsum(sample))\n",
    "    samplesizes = range(1, n+1,1)\n",
    "    return [RR(csSample[i])/samplesizes[i] for i in range(n)]\n",
    "\n",
    "def paretoFInverse(u,a=1.5):\n",
    "    '''A function to evaluate the inverse CDF of a Pareto type II (Lomax) distribution with parameter a and scale 1'''\n",
    "    return RR((1-u)^(-1/a)-1)\n",
    "\n",
    "def paretoSample(n,a=1.5):\n",
    "    '''A function to simulate samples from a Pareto type II (Lomax) distribution with parameter a and scale 1.\n",
    "    \n",
    "    Param n is the number of samples to simulate.'''\n",
    "    \n",
    "    us = [random() for i in range(n)]\n",
    "    return [paretoFInverse(u,a) for u in us]\n",
    "\n",
    "def paretoRunningMeans(n,a=1.5):\n",
    "    '''Function to give a list of n running means from a Pareto type II (Lomax) distribution with parameter a and scale 1.\n",
    "    \n",
    "    Param n is the number of running means to generate.'''\n",
    "    \n",
    "    sample = paretoSample(n,a)\n",
    "    from pylab import cumsum\n",
    "    csSample = list(cumsum(sample))\n",
    "    samplesizes = range(1, n+1,1)\n",
    "    return [RR(csSample[i])/samplesizes[i] for i in range(n)]\n",
    "\n",
    "def twoRunningMeansPlot(nToPlot, iters):\n",
    "    '''Function to return a graphics array containing plots of running means for Bernoulli and Standard Cauchy.\n",
    "    \n",
    "    Param nToPlot is the number of running means to simulate for each iteration.\n",
    "    Param iters is the number of iterations or sequences of running means or lines on each plot to draw.\n",
    "    Returns a graphics array object containing both plots with titles.'''\n",
    "    xvalues = range(1, nToPlot+1,1)\n",
    "    for i in range(iters):\n",
    "        shade = 0.5*(iters - 1 - i)/iters # to get different colours for the lines\n",
    "        bRunningMeans = bernoulliSecretThetaRunningMeans(nToPlot)\n",
    "        cRunningMeans = cauchyRunningMeans(nToPlot)\n",
    "        bPts = zip(xvalues, bRunningMeans)\n",
    "        cPts = zip(xvalues, cRunningMeans)\n",
    "        if (i < 1):\n",
    "            p1 = line(bPts, rgbcolor = (shade, 0, 1))\n",
    "            p2 = line(cPts, rgbcolor = (1-shade, 0, shade))\n",
    "            cauchyTitleMax = max(cRunningMeans) # for placement of cauchy title\n",
    "        else:\n",
    "            p1 += line(bPts, rgbcolor = (shade, 0, 1))\n",
    "            p2 += line(cPts, rgbcolor = (1-shade, 0, shade))\n",
    "            if max(cRunningMeans) > cauchyTitleMax: cauchyTitleMax = max(cRunningMeans)\n",
    "    titleText1 = \"Bernoulli running means\" # make title text\n",
    "    t1 = text(titleText1, (nToGenerate/2,1), rgbcolor='blue',fontsize=10) \n",
    "    titleText2 = \"Standard Cauchy running means\" # make title text\n",
    "    t2 = text(titleText2, (nToGenerate/2,ceil(cauchyTitleMax)+1), rgbcolor='red',fontsize=10)\n",
    "    return graphics_array((p1+t1,p2+t2))\n",
    "\n",
    "def pmfPointMassPlot(theta):\n",
    "    '''Returns a pmf plot for a point mass function with parameter theta.'''\n",
    "    \n",
    "    ptsize = 10\n",
    "    linethick = 2\n",
    "    fudgefactor = 0.07 # to fudge the bottom line drawing\n",
    "    pmf = points((theta,1), rgbcolor=\"blue\", pointsize=ptsize)\n",
    "    pmf += line([(theta,0),(theta,1)], rgbcolor=\"blue\", linestyle=':')\n",
    "    pmf += points((theta,0), rgbcolor = \"white\", faceted = true, pointsize=ptsize)\n",
    "    pmf += line([(min(theta-2,-2),0),(theta-0.05,0)], rgbcolor=\"blue\",thickness=linethick)\n",
    "    pmf += line([(theta+.05,0),(theta+2,0)], rgbcolor=\"blue\",thickness=linethick)\n",
    "    pmf+= text(\"Point mass f\", (theta,1.1), rgbcolor='blue',fontsize=10)\n",
    "    pmf.axes_color('grey') \n",
    "    return pmf\n",
    "    \n",
    "def cdfPointMassPlot(theta):\n",
    "    '''Returns a cdf plot for a point mass function with parameter theta.'''\n",
    "    \n",
    "    ptsize = 10\n",
    "    linethick = 2\n",
    "    fudgefactor = 0.07 # to fudge the bottom line drawing\n",
    "    cdf = line([(min(theta-2,-2),0),(theta-0.05,0)], rgbcolor=\"blue\",thickness=linethick) # padding\n",
    "    cdf += points((theta,1), rgbcolor=\"blue\", pointsize=ptsize)\n",
    "    cdf += line([(theta,0),(theta,1)], rgbcolor=\"blue\", linestyle=':')\n",
    "    cdf += line([(theta,1),(theta+2,1)], rgbcolor=\"blue\", thickness=linethick) # padding\n",
    "    cdf += points((theta,0), rgbcolor = \"white\", faceted = true, pointsize=ptsize)\n",
    "    cdf+= text(\"Point mass F\", (theta,1.1), rgbcolor='blue',fontsize=10)\n",
    "    cdf.axes_color('grey') \n",
    "    return cdf\n",
    "    \n",
    "def uniformFInverse(u, theta1, theta2):\n",
    "    '''A function to evaluate the inverse CDF of a uniform(theta1, theta2) distribution.\n",
    "    \n",
    "    u, u should be 0 <= u <= 1, is the value to evaluate the inverse CDF at.\n",
    "    theta1, theta2, theta2 > theta1, are the uniform distribution parameters.'''\n",
    "    \n",
    "    return theta1 + (theta2 - theta1)*u\n",
    "\n",
    "def uniformSample(n, theta1, theta2):\n",
    "    '''A function to simulate samples from a uniform distribution.\n",
    "    \n",
    "    n > 0 is the number of samples to simulate.\n",
    "    theta1, theta2 (theta2 > theta1) are the uniform distribution parameters.'''\n",
    "    \n",
    "    us = [random() for i in range(n)]\n",
    "    \n",
    "    return [uniformFInverse(u, theta1, theta2) for u in us]\n",
    "\n",
    "def exponentialFInverse(u, lam):\n",
    "    '''A function to evaluate the inverse CDF of a exponential distribution.\n",
    "    \n",
    "    u is the value to evaluate the inverse CDF at.\n",
    "    lam is the exponential distribution parameter.'''\n",
    "    \n",
    "    # log without a base is the natural logarithm\n",
    "    return (-1.0/lam)*log(1 - u)\n",
    "    \n",
    "def exponentialSample(n, lam):\n",
    "    '''A function to simulate samples from an exponential distribution.\n",
    "    \n",
    "    n is the number of samples to simulate.\n",
    "    lam is the exponential distribution parameter.'''\n",
    "    \n",
    "    us = [random() for i in range(n)]\n",
    "    \n",
    "    return [exponentialFInverse(u, lam) for u in us]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070f1a2",
   "metadata": {},
   "source": [
    "To get back to our running means of Bernoullin RVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "207feece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulliSecretThetaRunningMeans(n, mySeed = None):\n",
    "    '''Function to give a list of n running means from Bernoulli with unknown theta.\n",
    "    \n",
    "    Param n is the number of running means to generate.\n",
    "    Param mySeed is a value for the seed of the random number generator, defaulting to None\n",
    "    Note: the unknown theta parameter for the Bernoulli process is defined in bernoulliSampleSecretTheta\n",
    "    Return a list of n running means.'''\n",
    "    \n",
    "    sample = bernoulliSampleSecretTheta(n, simSeed = mySeed)\n",
    "    from pylab import cumsum # we can import in the middle of code\n",
    "    csSample = list(cumsum(sample))\n",
    "    samplesizes = range(1, n+1,1)\n",
    "    return [RR(csSample[i])/samplesizes[i] for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63506760",
   "metadata": {},
   "source": [
    "Now we can use this function to look at say 5 different sequences of running means (they will be different, because for each iteration, we will simulate a different sample of $Bernoulli$ observations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "048d8354",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m      5\u001b[0m     redshade \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(iterations \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m i)\u001b[38;5;241m/\u001b[39miterations \u001b[38;5;66;03m# to get different colours for the lines\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     bRunningMeans \u001b[38;5;241m=\u001b[39m \u001b[43mbernoulliSecretThetaRunningMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnToGenerate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(xvalues,bRunningMeans)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mbernoulliSecretThetaRunningMeans\u001b[1;34m(n, mySeed)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbernoulliSecretThetaRunningMeans\u001b[39m(n, mySeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m'''Function to give a list of n running means from Bernoulli with unknown theta.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Param n is the number of running means to generate.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Param mySeed is a value for the seed of the random number generator, defaulting to None\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Note: the unknown theta parameter for the Bernoulli process is defined in bernoulliSampleSecretTheta\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Return a list of n running means.'''\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mbernoulliSampleSecretTheta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimSeed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmySeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpylab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cumsum \u001b[38;5;66;03m# we can import in the middle of code\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     csSample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cumsum(sample))\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mbernoulliSampleSecretTheta\u001b[1;34m(n, theta, simSeed)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m'''A function to simulate samples from a bernoulli distribution.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mParam n is the number of samples to simulate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mParam theta is the bernoulli distribution parameter.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03mParam simSeed is a seed for the random number generator, defaulting to 30.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mReturns a simulated Bernoulli sample as a list.'''\u001b[39;00m\n\u001b[0;32m     45\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(simSeed)\n\u001b[1;32m---> 46\u001b[0m us \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[0;32m     47\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [bernoulliFInverse(u, theta) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m us]\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m'''A function to simulate samples from a bernoulli distribution.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mParam n is the number of samples to simulate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mParam theta is the bernoulli distribution parameter.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03mParam simSeed is a seed for the random number generator, defaulting to 30.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mReturns a simulated Bernoulli sample as a list.'''\u001b[39;00m\n\u001b[0;32m     45\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(simSeed)\n\u001b[1;32m---> 46\u001b[0m us \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[0;32m     47\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [bernoulliFInverse(u, theta) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m us]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "nToGenerate = 1500\n",
    "iterations = 5\n",
    "xvalues = range(1, nToGenerate+1,1)\n",
    "for i in range(iterations):\n",
    "    redshade = 0.5*(iterations - 1 - i)/iterations # to get different colours for the lines\n",
    "    bRunningMeans = bernoulliSecretThetaRunningMeans(nToGenerate)\n",
    "    pts = zip(xvalues,bRunningMeans)\n",
    "    if (i == 0):\n",
    "        p = line(pts, rgbcolor = (redshade,0,1))\n",
    "    else:\n",
    "        p += line(pts, rgbcolor = (redshade,0,1))\n",
    "show(p, figsize=[5,3], axes_labels=['n','sample mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49984a2",
   "metadata": {},
   "source": [
    "What we notice is how the different lines **converge** on a sample mean of close to 0.3. \n",
    "\n",
    "Is life always this easy?  Unfortunately no.  In the plot below we show the well-behaved running means for the $Bernoulli$ and beside them the running means for simulated  standard $Cauchy$ random variables.  They are all over the place, and each time you re-evaluate the cell you'll get different all-over-the-place behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nToGenerate = 15000\n",
    "iterations = 5\n",
    "g = twoRunningMeansPlot(nToGenerate, iterations) # uses above function to make plot\n",
    "show(g,figsize=[10,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bde9a",
   "metadata": {},
   "source": [
    "We talked about the Cauchy in more detail in an earlier notebook.  If you cannot recall the detail and are interested, go back to that in your own time.  The message here is that although with the Bernoulli process, the sample means converge as the number of observations increases, with the Cauchy they do not. \n",
    "\n",
    "\n",
    "\n",
    "## Limits of a Sequence of Real Numbers\n",
    "\n",
    "A sequence of real numbers $x_1, x_2, x_3, \\ldots $ (which we can also write as $\\{ x_i\\}_{i=1}^\\infty$) is said to converge to a limit $a \\in \\mathbb{R}$,\n",
    "\n",
    "$$\\underset{i \\rightarrow \\infty}{\\lim} x_i = a$$\n",
    "\n",
    "if for every natural number $m \\in \\mathbb{N}$, a natural number $N_m \\in \\mathbb{N}$ exists such that for every $j \\geq N_m$, $\\left|x_j - a\\right| \\leq \\frac{1}{m}$\n",
    "\n",
    "What is this saying? $\\left|x_j - a\\right|$ is measuring the closeness of the $j$th value in the sequence to $a$.  If we pick bigger and bigger $m$, $\\frac{1}{m}$ will get smaller and smaller.  The definition of the limit is saying that if $a$ is the limit of the sequence then we can get the sequence to become as close as we want ('arbitrarily close') to $a$, and to stay that close, by going far enough into the sequence ('for every $j \\geq N_m$, $\\left|x_j - a\\right| \\leq \\frac{1}{m}$')\n",
    "\n",
    "($\\mathbb{N}$, the natural numbers, are just the 'counting numbers' $\\{1, 2, 3, \\ldots\\}$.)\n",
    "\n",
    " \n",
    "\n",
    "Take a trivial example, the sequence $\\{x_i\\}_{i=1}^\\infty = 17, 17, 17, \\ldots$\n",
    "\n",
    "Clearly, $\\underset{i \\rightarrow \\infty}{\\lim} x_i = 17$, but let's do this formally:\n",
    "\n",
    "For every $m \\in \\mathbb{N}$, take $N_m =1$, then\n",
    "\n",
    "$\\forall$ $j \\geq N_m=1, \\left|x_j -17\\right| = \\left|17 - 17\\right| = 0 \\leq \\frac{1}{m}$, as required.\n",
    "\n",
    "($\\forall$ is mathspeak for 'for all' or 'for every')\n",
    "\n",
    "\n",
    "\n",
    "What about $\\{x_i\\}_{i=1}^\\infty = \\displaystyle\\frac{1}{1}, \\frac{1}{2}, \\frac{1}{3}, \\ldots$, i.e., $x_i = \\frac{1}{i}$?\n",
    "\n",
    "$\\underset{i \\rightarrow \\infty}{\\lim} x_i = \\underset{i \\rightarrow \\infty}{\\lim}\\frac{1}{i} = 0$\n",
    "\n",
    "For every $m \\in \\mathbb{N}$, take $N_m = m$, then $\\forall$ $j \\geq m$, $\\left|x_j - 0\\right| \\leq \\left |\\frac{1}{m} - 0\\right| = \\frac{1}{m}$\n",
    "\n",
    "## YouTry\n",
    "\n",
    "Think about $\\{x_i\\}_{i=1}^\\infty = \\frac{1}{1^p}, \\frac{1}{2^p}, \\frac{1}{3^p}, \\ldots$ with $p > 0$. The limit$\\underset{i \\rightarrow \\infty}{\\lim} \\displaystyle\\frac{1}{i^p} = 0$, provided $p > 0$.\n",
    "\n",
    "You can draw the plot of this very easily using the Sage symbolic expressions we have already met (`f.subs(...)` allows us to substitute a particular value for one of the symbolic variables in the symbolic function `f`, in this case a value to use for $p$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07539ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "var('i, p')\n",
    "f = 1/(i^p)\n",
    "# make and show plot, note we can use f in the label\n",
    "plot(f.subs(p=1), (x, 0.1, 3), axes_labels=('i',f)).show(figsize=[6,3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3aed07",
   "metadata": {},
   "source": [
    "What about $\\{x_i\\}_{i=1}^\\infty = 1^{\\frac{1}{1}}, 2^{\\frac{1}{2}}, 3^{\\frac{1}{3}}, \\ldots$. The limit$\\underset{i \\rightarrow \\infty}{\\lim} i^{\\frac{1}{i}} = 1$.\n",
    "\n",
    "This one is not as easy to see intuitively, but again we can plot it with SageMath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "var('i')\n",
    "f = i^(1/i)\n",
    "n=500\n",
    "p=plot(f.subs(p=1), (x, 0, n), axes_labels=('i',f)) # main plot\n",
    "p+=line([(0,1),(n,1)],linestyle=':') # add a dotted line at height 1\n",
    "p.show(figsize=[6,3]) # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8c3fd",
   "metadata": {},
   "source": [
    "Finally, $\\{x_i\\}_{i=1}^\\infty = p^{\\frac{1}{1}}, p^{\\frac{1}{2}}, p^{\\frac{1}{3}}, \\ldots$, with $p > 0$. The limit$\\underset{i \\rightarrow \\infty}{\\lim} p^{\\frac{1}{i}} = 1$ provided $p > 0$.\n",
    "\n",
    "You can cut and paste (with suitable adaptations) to try to plot this one as well ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ba453",
   "metadata": {},
   "source": [
    "(end of You Try)\n",
    "\n",
    "---\n",
    "\n",
    "*back to the real stuff ...*\n",
    "\n",
    "## Limits of Functions\n",
    "\n",
    "We say that a function $f(x): \\mathbb{R} \\rightarrow \\mathbb{R}$ has a limit $L \\in \\mathbb{R}$ as $x$ approaches $a$:\n",
    "\n",
    "$$\\underset{x \\rightarrow a}{\\lim} f(x) = L$$\n",
    "\n",
    "provided $f(x)$ is arbitrarily close to $L$ for all ($\\forall$) values of $x$ that are sufficiently close to but not equal to $a$.\n",
    "\n",
    "For example\n",
    "\n",
    "Consider the function $f(x) = (1+x)^{\\frac{1}{x}}$\n",
    "\n",
    "$\\underset{x \\rightarrow 0}{\\lim} f(x) = \\underset{x \\rightarrow 0}{\\lim} (1+x)^{\\frac{1}{x}} = e \\approx 2.71828\\cdots$\n",
    "\n",
    "even though $f(0) = (1+0)^{\\frac{1}{0}}$ is undefined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is defined as a symbolic variable by default by Sage so we do not need var('x')\n",
    "f = (1+x)^(1/x)\n",
    "# uncomment and try evaluating next line\n",
    "#f.subs(x=0) # this will give you an error message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09eccc7",
   "metadata": {},
   "source": [
    "You can get some idea of what is going on with two plots on different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (1+x)^(1/x)\n",
    "n1=5\n",
    "p1=plot(f.subs(p=1), (x, 0.001, n1), axes_labels=('x',f)) # main plot\n",
    "t1 = text(\"Large scale plot\", (n1/2,e), rgbcolor='blue',fontsize=10) \n",
    "n2=0.1\n",
    "p2=plot(f.subs(p=1), (x, 0.0000001, n2), axes_labels=('x',f)) # main plot\n",
    "p2+=line([(0,e),(n2,e)],linestyle=':') # add a dotted line at height e\n",
    "t2 = text(\"Small scale plot\", (n2/2,e+.01), rgbcolor='blue',fontsize=10) \n",
    "show(graphics_array((p1+t1,p2+t2)),figsize=[6,3]) # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea18fd",
   "metadata": {},
   "source": [
    "all this has been laying the groundwork for the topic of real interest to us ...\n",
    "\n",
    "## Limit of a Sequence of Random Variables\n",
    "\n",
    "We want to be able to say things like $\\underset{i \\rightarrow \\infty}{\\lim} X_i = X$ in some sensible way.  $X_i$ are some random variables, $X$ is some 'limiting random variable', but what do we mean by 'limiting random variable'?\n",
    "\n",
    "To help us, lets introduce a very very simple random variable, one that puts all its mass in one place.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2.0\n",
    "show(graphics_array((pmfPointMassPlot(theta),cdfPointMassPlot(theta))),\\\n",
    "     figsize=[8,2]) # show the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12b89b",
   "metadata": {},
   "source": [
    "This is known as the $Point\\,Mass(\\theta)$ random variable, $\\theta \\in \\mathbb(R)$:  the density $f(x)$ is 1 if $x=\\theta$ and 0 everywhere else\n",
    "\n",
    "$$\n",
    "f(x;\\theta) =\n",
    "\\begin{cases}\n",
    "0 & \\text{ if  }  x \\neq \\theta \\\\\n",
    "1 & \\text{ if  } x = \\theta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F(x;\\theta) =\n",
    "\\begin{cases}\n",
    "0 & \\text{ if  } x < \\theta \\\\\n",
    "1 & \\text{ if  } x \\geq \\theta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So, if we had some sequence $\\{\\theta_i\\}_{i=1}^\\infty$ and $\\underset{i \\rightarrow \\infty}{\\lim} \\theta_i = \\theta$\n",
    "\n",
    "and we had a sequence of random variables $X_i \\sim Point\\,Mass(\\theta_i)$, $i = 1, 2, 3, \\ldots$\n",
    "\n",
    "then we could talk about a limiting random variable as $X \\sim Point\\,Mass(\\theta)$:\n",
    "\n",
    "i.e., we could talk about $\\underset{i \\rightarrow \\infty}{\\lim} X_i = X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock up a picture of a sequence of point mass rvs converging on theta = 0\n",
    "ptsize = 20\n",
    "i = 1\n",
    "theta_i = 1/i\n",
    "p = points((theta_i,1), rgbcolor=\"blue\", pointsize=ptsize)\n",
    "p += line([(theta_i,0),(theta_i,1)], rgbcolor=\"blue\", linestyle=':')\n",
    "while theta_i > 0.01:\n",
    "    i+=1\n",
    "    theta_i = 1/i\n",
    "    p += points((theta_i,1), rgbcolor=\"blue\", pointsize=ptsize)\n",
    "    p += line([(theta_i,0),(theta_i,1)], rgbcolor=\"blue\", linestyle=':')\n",
    "p += points((0,1), rgbcolor=\"red\", pointsize=ptsize)\n",
    "p += line([(0,0),(0,1)], rgbcolor=\"red\", linestyle=':')\n",
    "p.show(xmin=-1, xmax = 2, ymin=0, ymax = 1.1, axes=false, gridlines=[None,[0]], \\\n",
    "       figsize=[7,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886e2ba",
   "metadata": {},
   "source": [
    "Now, we want to generalise this notion of a limit to other random variables (that are not necessarily $Point\\,Mass(\\theta_i)$ RVs)\n",
    "\n",
    "What about one many of you will be familiar with - the 'bell-shaped curve' \n",
    "\n",
    "### The $Gaussian(\\mu, \\sigma^2)$ or $Normal(\\mu, \\sigma^2)$ RV?\n",
    "\n",
    "The probability density function (PDF) $f(x)$ is given by\n",
    "\n",
    "$$\n",
    "f(x ;\\mu, \\sigma) = \\displaystyle\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(\\frac{-1}{2\\sigma^2}(x-\\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "The two parameters, $\\mu \\in \\mathbb{R} := (-\\infty,\\infty)$ and $\\sigma \\in (0,\\infty)$, are sometimes referred to as the location and scale parameters.\n",
    "\n",
    "To see why this is, use the interactive plot below to have a look at what happens to the shape of the density function $f(x)$ when you change $\\mu$ or increase or decrease $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def _(my_mu=input_box(0, label='mu') ,my_sigma=input_box(1,label='sigma')):\n",
    "    '''Interactive function to plot the normal pdf and ecdf.'''\n",
    "    \n",
    "    if my_sigma > 0:\n",
    "        html('<h4>Normal('+str(my_mu)+','+str(my_sigma)+'<sup>2</sup>)</h4>')\n",
    "        var('mu sigma')\n",
    "        f = (1/(sigma*sqrt(2.0*pi)))*exp(-1.0/(2*sigma^2)*(x - mu)^2)\n",
    "        p1=plot(f.subs(mu=my_mu,sigma=my_sigma), \\\n",
    "                (x, my_mu - 3*my_sigma - 2, my_mu + 3*my_sigma + 2),\\\n",
    "                axes_labels=('x','f(x)'))\n",
    "        show(p1,figsize=[8,3])\n",
    "    else:\n",
    "        print( \"sigma must be greater than 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f35351",
   "metadata": {},
   "source": [
    "Consider the sequence of random variables $X_1, X_2, X_3, \\ldots$, where\n",
    "\n",
    "- $X_1 \\sim Normal(0, 1)$\n",
    "- $X_2 \\sim Normal(0, \\frac{1}{2})$\n",
    "- $X_3 \\sim Normal(0, \\frac{1}{3})$\n",
    "- $X_4 \\sim Normal(0, \\frac{1}{4})$\n",
    "- $\\vdots$\n",
    "- $X_i \\sim Normal(0, \\frac{1}{i})$\n",
    "- $\\vdots$\n",
    "\n",
    "We can use the animation below to see how the PDF $f_{i}(x)$ looks as we move through the sequence of $X_i$ (the animation only goes to $i = 25$, $\\sigma = 0.04$ but you get the picture ...)\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Normal curve animation, looping through $\\sigma = \\frac{1}{i}$ for $i = 1, \\dots, 25$</th>\n",
    "  </tr>\n",
    "<tr>\n",
    "    <th><img src=\"images/normalDecreasing.gif\" width=\"300\"/></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "We can see that the probability mass of $X_i \\sim Normal(0, \\frac{1}{i})$ increasingly concentrates about 0 as $i \\rightarrow \\infty$ and $\\frac{1}{i} \\rightarrow 0$\n",
    "\n",
    "Does this mean that $\\underset{i \\rightarrow \\infty}{\\lim} X_i = Point\\,Mass(0)$?\n",
    "\n",
    "No, because for any $i$, however large, $P(X_i = 0) = 0$ because $X_i$ is a continuous RV (for any continous RV $X$, for any $x \\in \\mathbb{R}$, $P(X=x) = 0$).\n",
    "\n",
    "So, we need to refine our notions of convergence when we are dealing with random variables\n",
    "\n",
    "### Convergence in Distribution\n",
    "\n",
    "Let $X_1, X_2, \\ldots$ be a sequence of random variables and let $X$ be another random variable.  Let $F_i$ denote the distribution function (DF) of $X_i$ and let $F$ denote the distribution function of $X$.\n",
    "\n",
    "Now, if for any real number $t$ at which $F$ is continuous,\n",
    "\n",
    "$$\\underset{i \\rightarrow \\infty}{\\lim} F_i(t) = F(t)$$\n",
    "\n",
    "(in the sense of the convergence or limits of functions we talked about earlier)\n",
    "\n",
    "Then we can say that the sequence or RVs $X_i$, $i = 1, 2, \\ldots$ **converges to $X$ in distribution** and write $X_i \\rightsquigarrow X$ or $X_i \\overset{d}{\\rightarrow} X$.\n",
    "\n",
    "An equivalent way of defining convergence in distribution is to go right back to the meaning of the probabilty space 'under the hood' of a random variable, a random variable $X$ as a mapping from the sample space $\\Omega$ to the real line ($X: \\Omega \\rightarrow \\mathbb{R}$), and the sample points or outcomes in the sample space, the $\\omega \\in \\Omega$.  For $\\omega \\in \\Omega$, $X(\\omega)$ is the mapping of $\\omega$ to the real line $\\mathbb{R}$.  We could look at the set of $\\omega$ such that $X(\\omega) \\leq t$, i.e. the set of $\\omega$ that map to some value on the real line less than or equal to $t$, $\\{\\omega: X(\\omega) \\leq t \\}$. \n",
    "\n",
    "Saying that for any $t \\in \\mathbb{R}$, $\\underset{i \\rightarrow \\infty}{\\lim} F_i(t) = F(t)$ is the equivalent of saying that for any $t \\in \\mathbb{R}$, \n",
    "\n",
    "$$\\underset{i \\rightarrow \\infty}{\\lim} P\\left(\\{\\omega:X_i(\\omega) \\leq t \\}\\right) = P\\left(\\{\\omega: X(\\omega) \\leq t\\right)$$\n",
    "\n",
    "Armed with this, we can go back to our sequence of $Normal$ random variables  $X_1, X_2, X_3, \\ldots$, where\n",
    "\n",
    "- $X_1 \\sim Normal(0, 1)$\n",
    "- $X_2 \\sim Normal(0, \\frac{1}{2})$\n",
    "- $X_3 \\sim Normal(0, \\frac{1}{3})$\n",
    "- $X_4 \\sim Normal(0, \\frac{1}{4})$\n",
    "- $\\vdots$\n",
    "- $X_i \\sim Normal(0, \\frac{1}{i})$\n",
    "- $\\vdots$\n",
    "\n",
    "and let $X \\sim Point\\,Mass(0)$,\n",
    "\n",
    "and say that the $X_i$ **converge in distribution** to the $x \\sim Point\\,Mass$ RV $X$,\n",
    "\n",
    "$$X_i \\rightsquigarrow X \\qquad \\text{or} \\qquad X_i \\overset{d}{\\rightarrow} X$$\n",
    "\n",
    "What we are saying with convergence in distribution, informally, is that as $i$ increases, we increasingly expect to see the next outcome in a sequence of random experiments becoming better and better modeled by the limiting random variable.  In this case, as $i$ increases, the $Point\\,Mass(0)$ is becoming a better and better model for the next outcome of a random experiment with outcomes $\\sim Normal(0,\\frac{1}{i})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae044b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock up a picture of a sequence of converging normal distributions\n",
    "my_mu = 0\n",
    "upper = my_mu + 5; lower = -upper;     # limits for plot\n",
    "var('mu sigma')\n",
    "stop_i = 12\n",
    "html('<h4>N(0,1) to N(0, 1/'+str(stop_i)+')</h4>')\n",
    "f = (1/(sigma*sqrt(2.0*pi)))*exp(-1.0/(2*sigma^2)*(x - mu)^2)\n",
    "p=plot(f.subs(mu=my_mu,sigma=1.0), (x, lower, upper), rgbcolor = (0,0,1))\n",
    "for i in range(2, stop_i, 1): # just do a few of them\n",
    "    shade = 1-11/i # make them different colours\n",
    "    p+=plot(f.subs(mu=my_mu,sigma=1/i), (x, lower, upper), rgbcolor = (1-shade, 0, shade))\n",
    "textOffset = -0.2 # offset for placement of text -  may need adjusting \n",
    "p+=text(\"0\",(0,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p+=text(str(upper.n(digits=2)),(upper,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p+=text(str(lower.n(digits=2)),(lower,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p.show(axes=false, gridlines=[None,[0]], figsize=[7,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d255ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock up a picture of a sequence of converging normal distributions\n",
    "my_mu = 0\n",
    "upper = my_mu + 5; lower = -upper;     # limits for plot\n",
    "var('mu sigma')\n",
    "stop_i = 12\n",
    "html('<h4>N(0,1) to N(0, 1/'+str(stop_i)+')</h4>')\n",
    "f = (1/2)*(1+erf((x - mu)/(sqrt(2)*sigma)))\n",
    "p=plot(f.subs(mu=my_mu,sigma=1.0), (x, lower, upper), rgbcolor = (0,0,1))\n",
    "for i in range(2, stop_i, 1): # just do a few of them\n",
    "    shade = 1-11/i # make them different colours\n",
    "    p+=plot(f.subs(mu=my_mu,sigma=1/i), (x, lower, upper), rgbcolor = (1-shade, 0, shade))\n",
    "textOffset = -0.2 # offset for placement of text -  may need adjusting \n",
    "p+=text(\"0\",(0,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p+=text(str(upper.n(digits=2)),(upper,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p+=text(str(lower.n(digits=2)),(lower,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p.show(axes=false, gridlines=[None,[0]], figsize=[7,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96742c",
   "metadata": {},
   "source": [
    "#### There is an interesting point to note about this convergence: \n",
    "\n",
    "We have said that the $X_i \\sim Normal(0,\\frac{1}{i})$ with distribution functions $F_i$ converge in distribution to $X \\sim Point\\,Mass(0)$ with distribution function $F$, which means that we must be able to show that for any real number $t$ at which $F$ is continuous,\n",
    "\n",
    "$$\\underset{i \\rightarrow \\infty}{\\lim} F_i(t) = F(t)$$\n",
    "\n",
    "Note that for any of the $X_i \\sim Normal(0, \\frac{1}{i})$, $F_i(0) = \\frac{1}{2}$, and also note that for $X \\sim Point,Mass(0)$, $F(0) = 1$, so clearly $F_i(0) \\neq F(0)$.  \n",
    "\n",
    "What has gone wrong?  \n",
    "\n",
    "Nothing:  we said that we had to be able to show that $\\underset{i \\rightarrow \\infty}{\\lim} F_i(t) = F(t)$ for any $t \\in \\mathbb{R}$ at which $F$ is continuous, but the $Point\\,Mass(0)$ distribution function $F$ is not continous at 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.0\n",
    "# show the plots\n",
    "show(graphics_array((pmfPointMassPlot(theta),cdfPointMassPlot(theta))),figsize=[8,2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8155b4",
   "metadata": {},
   "source": [
    "### Convergence in Probability\n",
    "\n",
    "Let $X_1, X_2, \\ldots$ be a sequence of random variables and let $X$ be another random variable.  Let $F_i$ denote the distribution function (DF) of$X_i$ and let $F$ denote the distribution function of $X$.\n",
    "\n",
    "Now, if for any real number $\\varepsilon > 0$,\n",
    "\n",
    "$$\\underset{i \\rightarrow \\infty}{\\lim} P\\left(|X_i - X| > \\varepsilon\\right) = 0$$\n",
    "\n",
    "Then we can say that the sequence $X_i$, $i = 1, 2, \\ldots$ **converges to $X$ in probability** and write $X_i \\overset{P}{\\rightarrow} X$.\n",
    "\n",
    "Or, going back again to the probability space 'under the hood' of a random variable,  we could look the way the $X_i$ maps each outcome $\\omega \\in \\Omega$, $X_i(\\omega)$, which is some point on the real line, and compare this to mapping $X(\\omega)$.   \n",
    "\n",
    "Saying that for any $\\varepsilon \\in \\mathbb{R}$, $\\underset{i \\rightarrow \\infty}{\\lim} P\\left(|X_i - X| > \\varepsilon\\right) = 0$ is the equivalent of saying that for any $\\varepsilon \\in \\mathbb{R}$, \n",
    "\n",
    "$$\\underset{i \\rightarrow \\infty}{\\lim} P\\left(\\{\\omega:|X_i(\\omega) - X(\\omega)| > \\varepsilon \\}\\right) = 0$$\n",
    "\n",
    "Informally, we are saying $X$ is a limit in probabilty if, by going far enough into the sequence $X_i$, we can ensure that the mappings $X_i(\\omega)$ and $X(\\omega)$ will be arbitrarily close to each other on the real line for all $\\omega \\in \\Omega$.\n",
    "\n",
    "**Note** that convergence in distribution is implied by convergence in probability: convergence in distribution is the weakest form of convergence; any sequence of RV's that converges in probability to some RV $X$ also converges in distribution to $X$ (but not necessarily vice versa).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock up a picture of a sequence of converging normal distributions\n",
    "my_mu = 0\n",
    "var('mu sigma')\n",
    "upper = 0.2; lower = -upper\n",
    "i = 20 # start part way into the sequence\n",
    "lim = 100 # how far to go\n",
    "stop_i = 12\n",
    "html('<h4>N(0,1/'+str(i)+') to N(0, 1/'+str(lim)+')</h4>')\n",
    "f = (1/(sigma*sqrt(2.0*pi)))*exp(-1.0/(2*sigma^2)*(x - mu)^2)\n",
    "p=plot(f.subs(mu=my_mu,sigma=1.0/i), (x, lower, upper), rgbcolor = (0,0,1))\n",
    "for j in range(i, lim+1, 4): # just do a few of them\n",
    "    shade = 1-(j-i)/(lim-i) # make them different colours\n",
    "    p+=plot(f.subs(mu=my_mu,sigma=1/j), (x, lower,upper), rgbcolor = (1-shade, 0, shade))\n",
    "textOffset = -1.5 # offset for placement of text -  may need adjusting \n",
    "p+=text(\"0\",(0,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p+=text(str(upper.n(digits=2)),(upper,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p+=text(str(lower.n(digits=2)),(lower,textOffset),fontsize = 10, rgbcolor='grey') \n",
    "p.show(axes=false, gridlines=[None,[0]], figsize=[7,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41177a31",
   "metadata": {},
   "source": [
    "For our sequence of $Normal$ random variables  $X_1, X_2, X_3, \\ldots$, where\n",
    "\n",
    "- $X_1 \\sim Normal(0, 1)$\n",
    "- $X_2 \\sim Normal(0, \\frac{1}{2})$\n",
    "- $X_3 \\sim Normal(0, \\frac{1}{3})$\n",
    "- $X_4 \\sim Normal(0, \\frac{1}{4})$\n",
    "- $\\vdots$\n",
    "- $X_i \\sim Normal(0, \\frac{1}{i})$\n",
    "- $\\vdots$\n",
    "\n",
    "and $X \\sim Point\\,Mass(0)$,\n",
    "\n",
    "It can be shown that the $X_i$ converge in probability to $X \\sim Point\\,Mass(0)$ RV $X$,\n",
    "\n",
    "$$X_i \\overset{P}{\\rightarrow} X$$\n",
    "\n",
    "Since we are going to be using Markovs inequality later, we might as well take a look at it here and prove it.\n",
    "\n",
    "#### Markov's inequality\n",
    "\n",
    "This is our first concentration inequality.\n",
    "\n",
    "Let $x$ be a nonnegative random variable. Then for $a > 0$,\n",
    "$$\n",
    "    P(X \\geq a) \\leq \\frac{E[X]}{a}\n",
    "$$\n",
    "\n",
    "#### Proof\n",
    "\n",
    "Let us begin by assuming that $X$ is a continuous random variable and let us write\n",
    "$$\n",
    "    P(X \\geq a) = \\int_{a}^\\infty f(x) dx = \\frac{1}{a} \\int_{a}^\\infty a f(x) dx \\leq \\frac{1}{a} \\int_{a}^\\infty x f(x) dx \\leq \\frac{1}{a} E[X] \\enspace.\n",
    "$$\n",
    "\n",
    "#### Convergence in probability of our sequence\n",
    "Lets now use it! Remember we need to prove\n",
    "\n",
    "$$\\underset{i \\rightarrow \\infty}{\\lim} P\\left(|X_i - X| > \\varepsilon\\right) = 0$$\n",
    "\n",
    "but $X = 0$ since its a $Point\\,Mass(0)$ r.v. so by Markov's inequality and by further assuming that $X$ has finite variance, we get:\n",
    "\n",
    "$$\n",
    "    P\\left(|X_i - X| \\geq \\varepsilon\\right) = P\\left(|X_i - X|^2 \\geq \\varepsilon^2 \\right) \\leq \\frac{E[|X_i-X|^2]}{\\varepsilon^2} = \\text{Var}[X_i]\\frac{1}{\\varepsilon^2} \\leq \\frac{1}{i}\\frac{1}{\\varepsilon^2} \\enspace.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showURL(url, ht=500):\n",
    "    \"\"\"Return an IFrame of the url to show in notebook with height ht\"\"\"\n",
    "    from IPython.display import IFrame\n",
    "    return IFrame(url, width='95%', height=ht)\n",
    "showURL('https://en.wikipedia.org/wiki/Convergence_of_random_variables') # also check out 'almost sure convergence'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba7dd8",
   "metadata": {},
   "source": [
    "## Some Basic Limit Laws in Statistics\n",
    "\n",
    "Intuition behind Law of Large Numbers and Central Limit Theorem\n",
    "\n",
    "Take a look at the Khan academy videos on the Law of Large Numbers and the Central Limit Theorem. This will give you a working idea of these theorems.  In the sequel, we will strive for a deeper understanding of these theorems on the basis of the two notions of convergence of sequences of random variables we just saw.\n",
    "   \n",
    "\n",
    "### Weak Law of Large Numbers\n",
    "\n",
    "Remember that a statistic is a random variable, so a sample mean is a random variable. If we are given a sequence of independent and identically distributed RVs, $X_1,X_2,\\ldots \\overset{IID}{\\sim} X_1$, then we can also think of a sequence of random variables $\\overline{X}_1, \\overline{X}_2, \\ldots, \\overline{X}_n, \\ldots$ ($n$ being the sample size). \n",
    "\n",
    "Since $X_1, X_2, \\ldots$ are $IID$, they all have the same expection, say $E(X_1)$ by convention.\n",
    "\n",
    "If $E(X_1)$ exists, then the sample mean $\\overline{X}_n$ converges in probability to $E(X_1)$ (i.e., to the expectatation of any one of the individual RVs):\n",
    "\n",
    "$$\n",
    "\\text{If} \\quad X_1,X_2,\\ldots \\overset{IID}{\\sim} X_1 \\ \\text{and if } \\ E(X_1) \\ \\text{exists, then } \\ \\overline{X}_n \\overset{P}{\\rightarrow} E(X_1) \\ .\n",
    "$$\n",
    "\n",
    "Going back to our definition of convergence in probability, we see that this means that for any real number $\\varepsilon > 0$, $\\underset{n \\rightarrow \\infty}{\\lim} P\\left(|\\overline{X}_n - E(X_1)| > \\varepsilon\\right) = 0$\n",
    "\n",
    "Informally, this means that means that, by taking larger and larger samples we can make the probability that the average of the observations is more than $\\varepsilon$ away from the expected value get smaller and smaller.\n",
    "\n",
    "Proof of this is beyond the scope of this course, but we have already seen it in action when we looked at the $Bernoulli$ running means.  Have another look, this time with only one sequence of running means.  You can increase $n$, the sample size, and change $\\theta$.  Note that the seed for the random number generator is also under your control.   This means that you can get replicable samples:  in particular, in this interact, when you increase the sample size it looks as though you are just adding more to an existing sample rather than starting from scratch with a new one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df188aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def _(nToGen=slider(1,1500,1,100,label='n'),my_theta=input_box(0.3,label='theta'),rSeed=input_box(1234,label='random seed')):\n",
    "    '''Interactive function to plot running mean for a Bernoulli with specified n, theta and random number seed.'''\n",
    "    \n",
    "    if my_theta >= 0 and my_theta <= 1:\n",
    "        html('<h4>Bernoulli('+str(my_theta.n(digits=2))+')</h4>')\n",
    "        xvalues = range(1, nToGen+1,1)\n",
    "        bRunningMeans = bernoulliRunningMeans(nToGen, myTheta=my_theta, mySeed=rSeed)\n",
    "        pts = zip(xvalues, bRunningMeans)\n",
    "        p = line(pts, rgbcolor = (0,0,1))\n",
    "        p+=line([(0,my_theta),(nToGen,my_theta)],linestyle=':',rgbcolor='grey')\n",
    "        show(p, figsize=[5,3], axes_labels=['n','sample mean'],ymax=1)\n",
    "    else:\n",
    "        print ('Theta must be between 0 and 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Pareto and Weak law of large numbers\n",
    "running_means = [paretoRunningMeans(10000,a=1.9) for i in range(100)]\n",
    "\n",
    "@interact\n",
    "def _(n=slider(1,9999,100,999,label='n'),rSeed=input_box(1234,label='random seed')):\n",
    "    '''Interactive function to plot distribution of running mean for a Cauchy random variable with specified n and random number seed.'''\n",
    "    \n",
    "    n_th_means = [rm[n] for rm in running_means]\n",
    "    p=histogram(n_th_means)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda9c7e",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "You have probably all heard of the Central Limit Theorem before, but now we can relate it to our definition of convergence in distribution.  \n",
    "\n",
    "Let $X_1,X_2,\\ldots \\overset{IID}{\\sim} X_1$ and suppose $E(X_1)$ and $V(X_1)$ both exist,\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i  \\overset{d}{\\rightarrow} X \\sim Normal \\left(E(X_1),\\frac{V(X_1)}{n} \\right)\n",
    "$$\n",
    "\n",
    "And remember $Z \\sim Normal(0,1)$?\n",
    "\n",
    "Consider $Z_n := \\displaystyle\\frac{\\overline{X}_n-E(\\overline{X}_n)}{\\sqrt{V(\\overline{X}_n)}} = \\displaystyle\\frac{\\sqrt{n} \\left( \\overline{X}_n -E(X_1) \\right)}{\\sqrt{V(X_1)}}$\n",
    "\n",
    "If $\\overline{X}_n = \\displaystyle\\frac{1}{n} \\displaystyle\\sum_{i=1}^n X_i  \\overset{d}{\\rightarrow}  X \\sim Normal \\left(E(X_1),\\frac{V(X_1)}{n} \\right)$, then $\\overline{X}_n -E(X_1)  \\overset{d}{\\rightarrow}  X-E(X_1) \\sim Normal \\left( 0,\\frac{V(X_1)}{n} \\right)$\n",
    "\n",
    "and $\\sqrt{n} \\left( \\overline{X}_n -E(X_1) \\right)  \\overset{d}{\\rightarrow}  \\sqrt{n} \\left( X-E(X_1) \\right) \\sim Normal \\left( 0,V(X_1) \\right)$\n",
    "\n",
    "so $Z_n := \\displaystyle \\frac{\\overline{X}_n-E(\\overline{X}_n)}{\\sqrt{V(\\overline{X}_n)}} = \\displaystyle\\frac{\\sqrt{n} \\left( \\overline{X}_n -E(X_1) \\right)}{\\sqrt{V(X_1)}}  \\overset{d}{\\rightarrow}  Z \\sim Normal \\left( 0,1 \\right)$\n",
    "\n",
    "Thus, for sufficiently large $n$ (say $n>30$), probability statements about $\\overline{X}_n$ can be approximated using the $Normal$ distribution. \n",
    "\n",
    "The beauty of the CLT, as you have probably seen from other courses, is that $\\overline{X}_n \\overset{d}{\\rightarrow} Normal \\left( E(X_1), \\frac{V(X_1)}{n} \\right)$ does not require the $X_i$ to be normally distributed. \n",
    "\n",
    "We can try this with our $Bernoulli$ RV generator.  First, a small number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4eec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, n, samples = 0.6, 10, 5 # concise way to set some variable values\n",
    "sampleMeans=[] # empty list\n",
    "for i in range(0, samples, 1):  # loop \n",
    "    thisMean = QQ(sum(bernoulliSample(n, theta)))/n # get a sample and find the mean\n",
    "    sampleMeans.append(thisMean) # add mean to the list of means\n",
    "sampleMeans    # disclose the sample means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7c751",
   "metadata": {},
   "source": [
    "You can use the interactive plot to increase the number of samples and make a histogram of the sample means.  According to the CLT, for lots of reasonably-sized samples we should get a nice symmetric bell-curve-ish histogram centred on $\\theta$.  You can adjust the number of bins in the histogram as well as the number of samples, sample size, and $\\theta$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab03b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "@interact\n",
    "def _(replicates=slider(1,3000,1,100,label='replicates'), \\\n",
    "      nToGen=slider(1,1500,1,100,label='sample size n'),\\\n",
    "      my_theta=input_box(0.3,label='theta'),Bins=5):\n",
    "    '''Interactive function to plot distribution of replicates of sample means for n IID Bernoulli trials.'''\n",
    "    \n",
    "    if my_theta >= 0 and my_theta <= 1 and replicates > 0:\n",
    "        sampleMeans=[] # empty list\n",
    "        for i in range(0, replicates, 1):          \n",
    "            thisMean = RR(sum(bernoulliSample(nToGen, my_theta)))/nToGen\n",
    "            sampleMeans.append(thisMean)\n",
    "        pylab.clf() # clear current figure\n",
    "        n, bins, patches = pylab.hist(sampleMeans, Bins, density=true) \n",
    "        pylab.ylabel('normalised count')\n",
    "        pylab.title('Normalised histogram for Bernoulli sample means')\n",
    "        pylab.savefig('myHist') # to actually display the figure\n",
    "        pylab.show()\n",
    "        #show(p, figsize=[5,3], axes_labels=['n','sample mean'],ymax=1)\n",
    "    else:\n",
    "        print ('Theta must be between 0 and 1, and samples > 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc80f35",
   "metadata": {},
   "source": [
    "Increase the sample size and the numbe rof bins in the above interact and see if the histograms of the sample means are looking more and more normal as the CLT would have us believe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7294ed",
   "metadata": {},
   "source": [
    "But although the $X_i$ do not have to be $\\sim Normal$ for $\\overline{X}_n = \\overset{d}{\\rightarrow} X \\sim Normal\\left(E(X_1),\\frac{V(X_1)}{n} \\right)$, remember that we said \"Let $X_1,X_2,\\ldots \\overset{IID}{\\sim} X_1$ and suppose $E(X_1)$ and $V(X_1)$ both exist\", then,\n",
    "\n",
    "$$\n",
    "\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i  \\overset{d}{\\rightarrow} X \\sim Normal \\left(E(X_1),\\frac{V(X_1)}{n} \\right)\n",
    "$$\n",
    "\n",
    "This is where is all goes horribly wrong for the standard $Cauchy$ distribution (any $Cauchy$ distribution in fact):  neither the expectation nor the variance exist for this distribution.  The Central Limit Theorem cannot be applied here.  In fact, if $X_1,X_2,\\ldots \\overset{IID}{\\sim}$ standard $Cauchy$, then $\\overline{X}_n = \\displaystyle \\frac{1}{n} \\sum_{i=1}^n X_i  \\sim$ standard $Cauchy$.\n",
    "\n",
    "## YouTry\n",
    "\n",
    "Try looking at samples from two other RVs where the expectation and variance do exist, the $Uniform$ and the $Exponential$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "@interact\n",
    "def _(replicates=input_box(100,label='replicates'), \\\n",
    "      nToGen=slider(1,1500,1,100,label='sample size n'),\\\n",
    "      my_theta1=input_box(2,label='theta1'),\\\n",
    "      my_theta2=input_box(4,label='theta1'),Bins=5):\n",
    "    '''Interactive function to plot distribution of \n",
    "    sample means for n IID Uniform(theta1, theta2) trials.'''\n",
    "    \n",
    "    if (my_theta1 < my_theta2) and replicates > 0:\n",
    "        sampleMeans=[] # empty list\n",
    "        for i in range(0, replicates, 1):\n",
    "            \n",
    "            thisMean = RR(sum(uniformSample(nToGen, my_theta1, my_theta2)))/nToGen\n",
    "            sampleMeans.append(thisMean)\n",
    "        pylab.clf() # clear current figure\n",
    "        n, bins, patches = pylab.hist(sampleMeans, Bins, density=true) \n",
    "        pylab.ylabel('normalised count')\n",
    "        pylab.title('Normalised histogram for Uniform sample means')\n",
    "        pylab.savefig('myHist') # to actually display the figure\n",
    "        pylab.show()\n",
    "        #show(p, figsize=[5,3], axes_labels=['n','sample mean'],ymax=1)\n",
    "    else:\n",
    "        print ('theta1 must be less than theta2, and samples > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506cc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "@interact\n",
    "def _(replicates=input_box(100,label='replicates'), \\\n",
    "      nToGen=slider(1,1500,1,100,label='sample size n'),\\\n",
    "      my_lambda=input_box(0.1,label='lambda'),Bins=5):\n",
    "    '''Interactive function to plot distribution of \\\n",
    "    sample means for an Exponential(lambda) process.'''\n",
    "    \n",
    "    if my_lambda > 0 and replicates > 0:\n",
    "        sampleMeans=[] # empty list\n",
    "        for i in range(0, replicates, 1):            \n",
    "            thisMean = RR(sum(exponentialSample(nToGen, my_lambda)))/nToGen\n",
    "            sampleMeans.append(thisMean)\n",
    "        pylab.clf() # clear current figure\n",
    "        n, bins, patches = pylab.hist(sampleMeans, Bins, density=true) \n",
    "        pylab.ylabel('normalised count')\n",
    "        pylab.title('Normalised histogram for Exponential sample means')\n",
    "        pylab.savefig('myHist') # to actually display the figure\n",
    "        pylab.show()\n",
    "        #show(p, figsize=[5,3], axes_labels=['n','sample mean'],ymax=1)\n",
    "    else:\n",
    "        print ('lambda must be greater than 0, and samples > 0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "lx_course_instance": "2022",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
